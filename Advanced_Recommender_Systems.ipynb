{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4l8kmPb9H_1Y"
   },
   "source": [
    "<center> <h1> LBSN Project </h1> </center>\n",
    "\n",
    "\n",
    "Authors: Cédric Allain, Clotilde Miura, Manon Rivoire, Adriano Del Gallo, Saousan Kaddami\n",
    "\n",
    "Group #4\n",
    "\n",
    "April 11th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this lab is to implement a location-based recommender system from regular location-based social network (LBSN ) data. Usually LBSN data give precise information regarding the locations visited by the users. These data also reveal who are the friends of the users. These two information (geographical + social) can be exploited into a standard collaborative filtering approach.\n",
    "\n",
    "We build in this notebook blocks of the model developped by Zhang and Chow, 2013, iGSLR: personalized geo-social location recommendation: a kernel density estimation approach.\n",
    "\n",
    "Source: https://dl.acm.org/doi/10.1145/2525314.2525339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtBO9YRYXH7p"
   },
   "outputs": [],
   "source": [
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xG5g2rOgH_1h"
   },
   "source": [
    "If you have a look here :  ",
    "https://jupyter-notebook.readthedocs.io/en/stable/config.html ",
    "You will see that the limit is 500Mo ",
    "\"NotebookApp.max_buffer_sizeInt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Qm3u0OBB4PSc",
    "outputId": "b23cbb3c-eb5d-46a4-cbee-c677e7133757"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jupyter notebook --NotebookApp.max_buffer_size=<64>'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"jupyter notebook --NotebookApp.max_buffer_size=<64>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "yvXLZnQDQUJP",
    "outputId": "729d155f-e03f-440d-cda4-5e0d89cf3fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in /Users/cedricallain/anaconda3/lib/python3.7/site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in /Users/cedricallain/anaconda3/lib/python3.7/site-packages (from surprise) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/cedricallain/anaconda3/lib/python3.7/site-packages (from scikit-surprise->surprise) (1.3.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/cedricallain/anaconda3/lib/python3.7/site-packages (from scikit-surprise->surprise) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /Users/cedricallain/anaconda3/lib/python3.7/site-packages (from scikit-surprise->surprise) (1.17.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/cedricallain/anaconda3/lib/python3.7/site-packages (from scikit-surprise->surprise) (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6LLvsXHH_1k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "import numpy as np \n",
    "import pickle\n",
    "import math \n",
    "from collections import defaultdict \n",
    "from tqdm import tqdm\n",
    "\n",
    "from surprise import AlgoBase\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "The input dataset for this lab was collected from Gowalla, a popular LBSN that is often used to test recommendation methods with geographical dimensions. In practice the dataset contains user profiles, user friendship, location profiles, and users’ check-in history made before June 1, 2011. It contains 36,001,959 check-ins made by 407,533 users over 2,724,891 locations.\n",
    "\n",
    "Data can be downloaded here: https://snap.stanford.edu/data/loc-gowalla.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './gowalla'\n",
      "/Users/cedricallain/Documents/ENSAE/3A/S2/Systèmes de recommandation/Projet/gowalla\n"
     ]
    }
   ],
   "source": [
    "if use_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=True)\n",
    "    %cd drive/'My Drive'/recommender_systems_2020/Data/gowalla\n",
    "else:\n",
    "    %cd './gowalla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataFrames\n",
    "\n",
    "checkins_df = pd.read_csv('gowalla_checkins.csv')\n",
    "friendship_df = pd.read_csv('gowalla_friendship.csv')\n",
    "locations_df = pd.read_csv('gowalla_locations.csv')\n",
    "users_df = pd.read_csv('gowalla_userinfo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_Hi0AiOH_1t"
   },
   "source": [
    "## 2. Processing of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5s--UEzhH_1v"
   },
   "source": [
    "- Surprise library cannot handle implicit feedback \n",
    "- sigmoidal transformation : convert a frequency into a specific range \n",
    "- goal is to generate recommendations based on the building blocks ==> surprise frameworks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewbqt58QXH8X",
    "outputId": "98fb9c10-6779-4565-83f3-3abeaf57326e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkins_df , shape: (36001959, 2)\n",
      "   userid  placeid\n",
      "0    1338   482954\n",
      "1    1338   580963\n",
      "2    1338   365256\n",
      "3    1338    89504\n",
      "4    1338  1267135 \n",
      "\n",
      "friendship_df , shape: (4418339, 2)\n",
      "   userid1  userid2\n",
      "0        1    63488\n",
      "1        1        2\n",
      "2        1        3\n",
      "3        1        4\n",
      "4        1        5 \n",
      "\n",
      "locations_df , shape: (2724891, 5)\n",
      "     id         lng        lat  checkins_count  users_count\n",
      "0  8904  -94.607499  39.052318             114           21\n",
      "1  8932  -97.254356  32.927662              67           48\n",
      "2  8936  -94.591995  39.053318              75           46\n",
      "3  8938  -94.590311  39.052824             438           94\n",
      "4  8947 -122.029631  37.331880            3100         1186 \n",
      "\n",
      "users_d , shape: (407533, 3)\n",
      "   id  friends_count  checkin_num\n",
      "0   1            372         1766\n",
      "1   2            775         2892\n",
      "2   3            100         3021\n",
      "3   4            179         1325\n",
      "4   5            525         3215 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_df = {'checkins_df': checkins_df,\n",
    "           'friendship_df': friendship_df,\n",
    "           'locations_df': locations_df,\n",
    "           'users_d': users_df}\n",
    "\n",
    "for name, df in dict_df.items():\n",
    "    print(name, ', shape:', df.shape)\n",
    "    print(df.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOXVTx15XH8b"
   },
   "source": [
    "### Filtering out some users and POIs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw_OouFGHL1W",
    "outputId": "f32838f5-f3b9-4068-c3bc-16f8dd72f925"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46981"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.checkin_num.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rmb_EkETXH8n"
   },
   "source": [
    "Problem:\n",
    "- This user made more than 46981 different checkins.\n",
    "- It means we would have to compute $\\begin{pmatrix}46981 \\\\\n",
    "                                                        2\\end{pmatrix}$ distances between the different pairs of locations this user visited, which is equal to more than 1 billion...\n",
    "\n",
    "Solution:\n",
    "- Filter out users who have less than 15 checkins and more than 100 checkins\n",
    "- Filter out POIs/locations which have less than 30 visitors\n",
    "\n",
    "Remark: a similar filter is done in Guo et al., 2019, Network Embedding-Aware Point-of-Interest Recommendation in Location-Based Social Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users: 82410\n",
      "number of locations: 93680\n"
     ]
    }
   ],
   "source": [
    "# filter out users who have less than 15 checkins  and more  than 100 checkins\n",
    "users_df = users_df[(users_df.checkin_num >= 15) & (users_df.checkin_num <=100)]\n",
    "users_list = users_df.id.tolist()\n",
    "\n",
    "# filter out POIs/locations which have less than 30 visitors\n",
    "locations_df = locations_df[locations_df.users_count >= 30]\n",
    "locations_list = locations_df.id.tolist()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    done = True\n",
    "    \n",
    "    old_users_list = users_list.copy()\n",
    "    old_locations_list = locations_list.copy()\n",
    "    \n",
    "    # filter friendship_df to only get friends of our n users\n",
    "    friendship_df = friendship_df[friendship_df['userid1'].isin(users_list)]\n",
    "    # but those friends must have more than 15 checkins and less than 100, i.e. be in our list of users\n",
    "    friendship_df = friendship_df[friendship_df['userid2'].isin(users_list)]\n",
    "    # recompute users list\n",
    "    users_list = friendship_df['userid1'].unique().tolist()\n",
    "    \n",
    "    # also filter the checkins dataframe\n",
    "    checkins_df = checkins_df[(checkins_df.userid.isin(users_list)) & (checkins_df.placeid.isin(locations_list))]\n",
    "    \n",
    "    # recompute users and locations list\n",
    "    users_list = checkins_df['userid'].unique().tolist()\n",
    "    locations_list = checkins_df['placeid'].unique().tolist()\n",
    "    \n",
    "    if (len(old_users_list)>len(users_list)) or (len(old_locations_list)>len(locations_list)):\n",
    "        done = False\n",
    "        \n",
    "# filter again users and locations data frames\n",
    "users_df = users_df[users_df.id.isin(users_list)]\n",
    "locations_df = locations_df[locations_df.id.isin(locations_list)]\n",
    "    \n",
    "print('number of users:', len(users_list))\n",
    "print('number of locations:', len(locations_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: people who only had friends who made less than 15 checkins ore more than 100 checkins are discarded, even if they made more than 15 checkins or less than 100 checkins themselves.\n",
    "\n",
    "Thus, restricting checkins_df to the remaining users in friendship_df and locations in the list of locations can discard some users, that is why we do a while loop (it only loops 2 times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KYi2M9lH_2M"
   },
   "source": [
    "### Build df_user_friends \n",
    "\n",
    "For each user, associate the list of his friends and put the result in a new dataframe df_user_friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS_KE45pH_2O",
    "outputId": "b910e426-3788-46c5-f328-f96e9df9fe1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape friendship_df: (401340, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid1</th>\n",
       "      <th>userid2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2886</td>\n",
       "      <td>20</td>\n",
       "      <td>28448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2889</td>\n",
       "      <td>20</td>\n",
       "      <td>9285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>20</td>\n",
       "      <td>338695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2898</td>\n",
       "      <td>2097174</td>\n",
       "      <td>334541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2899</td>\n",
       "      <td>2097174</td>\n",
       "      <td>2110997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid1  userid2\n",
       "2886       20    28448\n",
       "2889       20     9285\n",
       "2890       20   338695\n",
       "2898  2097174   334541\n",
       "2899  2097174  2110997"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape friendship_df:', friendship_df.shape)\n",
    "friendship_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuGw31FZH_2t",
    "outputId": "15f6317a-707e-461c-c21a-847fa80cec04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_user_friends: (82410, 2)\n",
      "CPU times: user 27.9 s, sys: 567 ms, total: 28.4 s\n",
      "Wall time: 30.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>list_friends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>[28448, 9285, 338695]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>[16643, 30855, 98447, 10014, 284839, 136274, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>[69506, 165411, 46163]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>[2372512, 276655, 2373417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>[108309]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid                                       list_friends\n",
       "0      20                              [28448, 9285, 338695]\n",
       "1      76  [16643, 30855, 98447, 10014, 284839, 136274, 2...\n",
       "2      81                             [69506, 165411, 46163]\n",
       "3      86                         [2372512, 276655, 2373417]\n",
       "4      93                                           [108309]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_user_friends = friendship_df.groupby('userid1').agg({'userid2': 'unique'})\n",
    "df_user_friends.reset_index(inplace=True)\n",
    "df_user_friends.columns = ['userid', 'list_friends']\n",
    "\n",
    "print('shape df_user_friends:', df_user_friends.shape)\n",
    "df_user_friends.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtAQlYyHXH9M"
   },
   "source": [
    "### Build df_user_locations\n",
    "\n",
    "Associate for each user the list of the successive locations he visited and put the result in a new dataframe df_user_locations.\n",
    "\n",
    "Remark: this dataframe is not used for the creation of df_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWOMzg-CXH9N",
    "outputId": "733820b2-888d-4c43-b425-1fa55ab6fba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_user_locations: (82410, 1)\n",
      "CPU times: user 23.2 s, sys: 287 ms, total: 23.5 s\n",
      "Wall time: 23.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_locations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>[69875, 1186843, 67957, 68209, 37322, 99630, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>[21076, 107498, 81300, 29778, 43297, 367123, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>[85577, 128873, 6708957, 759363, 7040412, 9226...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>[74468, 36326, 1333629, 38713, 6741441, 69870,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>[17831, 20918, 9220, 15245, 180328, 381380, 18...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           list_locations\n",
       "userid                                                   \n",
       "20      [69875, 1186843, 67957, 68209, 37322, 99630, 4...\n",
       "76      [21076, 107498, 81300, 29778, 43297, 367123, 1...\n",
       "81      [85577, 128873, 6708957, 759363, 7040412, 9226...\n",
       "86      [74468, 36326, 1333629, 38713, 6741441, 69870,...\n",
       "93      [17831, 20918, 9220, 15245, 180328, 381380, 18..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_user_locations = checkins_df.groupby('userid').agg({'placeid': 'unique'}) \n",
    "df_user_locations.columns = ['list_locations']\n",
    "\n",
    "print('shape df_user_locations:', df_user_locations.shape)\n",
    "df_user_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJMkxKJyXH9P"
   },
   "source": [
    "### Build df_frequencies\n",
    "\n",
    "Compute for each pair (user, location) its corresponding visit frequency in order to build a new df_frequencies dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wF-2wuAXH9Q",
    "outputId": "6b45152d-f1cb-462b-bc5f-cf7aa2f95615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_frequencies: (1084596, 3)\n",
      "CPU times: user 609 ms, sys: 68.5 ms, total: 677 ms\n",
      "Wall time: 549 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>placeid</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>9540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>9780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>9782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>9783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  placeid  freq\n",
       "0      20     9540     1\n",
       "1      20     9724     1\n",
       "2      20     9780     1\n",
       "3      20     9782     1\n",
       "4      20     9783     1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_frequencies = checkins_df.groupby(['userid', 'placeid']).agg({'placeid': 'count'})\n",
    "df_frequencies.columns = ['freq']\n",
    "df_frequencies.reset_index(inplace=True)\n",
    "\n",
    "print('shape df_frequencies:', df_frequencies.shape)\n",
    "df_frequencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AELG9cNGXH9T"
   },
   "source": [
    "Transform and update the frequencies from df_frequencies into the range [0, 10] with the following normalization transformation:\n",
    "\n",
    "$$x \\mapsto 10 \\cdot \\tanh \\left[10 \\cdot \\frac{x-f_{\\min }}{f_{\\max }-f_{\\min }}\\right]$$\n",
    "\n",
    "where $f_{min}$ and $f_{max}$ refers respectively to the minimum and maximum visit frequencies of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onl2szvNXH9U"
   },
   "outputs": [],
   "source": [
    "def normalize(x, fmin, fmax):\n",
    "    \"\"\"Normalization tranformation function applied on an integer\n",
    "    \n",
    "    Apply the following normalization function:\n",
    "    \n",
    "        10 * tanh(10 * (x-fmin)/(fmax-fmin))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: integer\n",
    "        The number we want to normalize.\n",
    "    \n",
    "    fmin: integer\n",
    "        The minimum visit frequencies of the dataset.\n",
    "    \n",
    "    fmax: integer\n",
    "        The maximum visit frequencies of the dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res: integer\n",
    "        input x normalized between 0 and 10\n",
    "    \n",
    "    \"\"\"\n",
    "    res = 10 * np.tanh(10 * (x-fmin)/(fmax-fmin+1e-16))\n",
    "    return res\n",
    "\n",
    "\n",
    "def normalize_serie(serie):\n",
    "    \"\"\"Normalization tranformation function applied on a serie of integers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    serie: pandas Series of integers\n",
    "        The serie we want to normalize\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    serie_normalized: pandas Series of integers\n",
    "        input serie normalized to contained only integers between 0 and 10\n",
    "    \n",
    "    \"\"\"\n",
    "    fmin = serie.min()\n",
    "    fmax = serie.max()\n",
    "    \n",
    "    serie_normalized = serie.apply(normalize, args=(fmin,fmax))\n",
    "    \n",
    "    return serie_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5OVm3gUXH9X",
    "outputId": "7423a114-d94f-4ae3-cefe-02864711eaf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.12 s, sys: 78.5 ms, total: 3.2 s\n",
      "Wall time: 3.24 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>placeid</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>9540</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>9780</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>9782</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>9783</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  placeid  freq\n",
       "0      20     9540   0.0\n",
       "1      20     9724   0.0\n",
       "2      20     9780   0.0\n",
       "3      20     9782   0.0\n",
       "4      20     9783   0.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_frequencies.freq = normalize_serie(df_frequencies.freq)\n",
    "df_frequencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZhab72YHL6i",
    "outputId": "dccb996c-5e19-4b02-b27f-61ebb0fa391c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.084596e+06\n",
       "mean     3.781687e-01\n",
       "std      1.126331e+00\n",
       "min      0.000000e+00\n",
       "10%      0.000000e+00\n",
       "50%      0.000000e+00\n",
       "75%      0.000000e+00\n",
       "90%      1.243530e+00\n",
       "95%      2.449187e+00\n",
       "max      1.000000e+01\n",
       "Name: freq, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frequencies.freq.describe(percentiles=[0.1, 0.5, 0.75, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yZ-6V8uHL6k"
   },
   "source": [
    "Remark: the normalization sets to 0 most of the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpSJenAJXH9j"
   },
   "source": [
    "## 3. Geographical Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uaKSC7LXH9j"
   },
   "source": [
    "Compute for each user $u$ the distances $d_{ij}$ between each pair of locations he has been located to.\n",
    "\n",
    "$$\\forall l_i, l_j \\in L_u, \\quad d_{ij} = \\text{distance}(l_i, l_j) $$\n",
    "\n",
    "where $L_u$ is, for an user $u$, the list of all locations he has been located to.\n",
    "\n",
    "To compute the distance between two spatial points, i.e. defined by their their latitude and longitude, we use the haversine distance: https://en.wikipedia.org/wiki/Haversine_formula\n",
    "\n",
    "The package haversine allow us to compute such a distance: https://github.com/mapado/haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vo5giIaxXH9j",
    "outputId": "d009cd12-473a-4bb8-858e-2d803bd5e32c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haversine in /Users/cedricallain/anaconda3/lib/python3.7/site-packages (2.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x):\n",
    "    \"\"\"Kernel function\"\"\"\n",
    "    k = np.exp(-(x**2)/2) / (np.sqrt(2*np.pi))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amcziAr5XH9t"
   },
   "outputs": [],
   "source": [
    "def density(new_d, locations_dist, n):\n",
    "    \"\"\"Compute the density of any new given distance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    new_d: integer\n",
    "        the new distance for which we want to compute the density\n",
    "        \n",
    "    locations_dist: list of integers\n",
    "        distances between pairs of locations\n",
    "        \n",
    "    n: double\n",
    "        number of locations visited by the user\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    f_hat: integer\n",
    "        the density computed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sigma_hat = np.std(locations_dist)\n",
    "    \n",
    "    h = 1.06 * sigma_hat * n**-1/5\n",
    "    \n",
    "    # compute density\n",
    "    num = np.sum([kernel((new_d - d)/h) for d in locations_dist])\n",
    "    den = len(locations_dist)*h\n",
    "    \n",
    "    if den==0:\n",
    "        f_hat = 0\n",
    "    else:\n",
    "        f_hat = num/den\n",
    "    \n",
    "    return f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDK2ktmOHL8w"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from haversine import haversine\n",
    "\n",
    "def geo_proba(user, new_loc, df_user_locations=df_user_locations, unit='km'): \n",
    "    \"\"\"Compute the geographical likelihood that a user will visit a new location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user: double\n",
    "        id of user\n",
    "\n",
    "    new_loc: integer ou tuple\n",
    "        if integer, id of the new location\n",
    "        if tuple, tuple (lat, lng) of the new location\n",
    "        \n",
    "    df_user_locations: pandas DataFrame\n",
    "        dataframe that associates for each user the list of the successive locations he visited\n",
    "        index: userid\n",
    "        columns = [list_locations]\n",
    "        \n",
    "    unit: string\n",
    "        the initials of its corresponding unit of measurement\n",
    "        default is 'km'\n",
    "        (kilometers = 'km', meters = 'm', miles = 'mi',\n",
    "        nautical miles = 'nmi', feet = 'ft', inches = 'in')\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    proba: integer\n",
    "        the geographical likelihood that the user will visit location new_loc\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # get list of locations visited by the user\n",
    "        list_locations = set(df_user_locations.loc[user, 'list_locations'])\n",
    "    except:\n",
    "        # if the user has visited no location, return 0\n",
    "        return 0\n",
    "        \n",
    "    n = len(list_locations) # number of unique locations visited by the user\n",
    "    \n",
    "    # get coordinates pair (lat, lng) for every locations visited by the user \n",
    "    coordinates_df = locations_df.set_index('id')[['lat', 'lng']]\n",
    "    coordinates_pairs = [(coordinates_df['lat'][loc], coordinates_df['lng'][loc])\n",
    "                         for loc in list_locations]\n",
    "    \n",
    "    if type(new_loc)==int:\n",
    "        # get tuple (lat, lng) for the new location\n",
    "        new_loc = tuple(locations_df.set_index('id').loc[new_loc, ['lat','lng']].tolist())\n",
    "    \n",
    "    # compute distances between the new location and every locations visited by the user \n",
    "    new_distances = [haversine(new_loc, loc, unit=unit)\n",
    "                     for loc in coordinates_pairs]\n",
    "    \n",
    "    locations_dist = [haversine(loc1, loc2, unit=unit)\n",
    "                      for loc1, loc2 in combinations(coordinates_pairs, 2)]\n",
    "   \n",
    "    # compute likelihood\n",
    "    proba = np.mean([density(new_d, locations_dist, n)\n",
    "                     for new_d in new_distances])\n",
    "    \n",
    "    return proba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7rpUq3pTHL8y",
    "outputId": "348f3d59-768e-40ae-cfbe-94b5250172e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 13.1 ms, total: 130 ms\n",
      "Wall time: 75.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "geo_proba(user=20, new_loc=219544)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kMRiEtWXH-A"
   },
   "source": [
    "## 4. Social computations\n",
    "\n",
    "For every pair of user $(u_i, u_k)$, compute their social similarity score with the Jaccard coefficient as follows:\n",
    "\n",
    "$$\\operatorname{sim}\\left(u_{i}, u_{k}\\right)=\\frac{\\left|F\\left(u_{i}\\right) \\cap F\\left(u_{k}\\right)\\right|}{\\left|F\\left(u_{i}\\right) \\cup F\\left(u_{k}\\right)\\right|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_friends = df_user_friends.set_index('userid')['list_friends'].to_dict()\n",
    "\n",
    "def social_similarity(user1, user2):\n",
    "    \"\"\"Compute Jaccard coefficient to obtain social similarity between two users\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user1, user2: double\n",
    "        id of the users\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sim: integer\n",
    "        social similarity between user1 and user2\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        num = np.intersect1d(dict_friends[user1], dict_friends[user2]).shape[0]\n",
    "        den = np.union1d(dict_friends[user1], dict_friends[user2]).shape[0]\n",
    "        if den==0:\n",
    "            sim = 0\n",
    "        else:\n",
    "            sim = num/den\n",
    "    except:\n",
    "        sim = 0\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score can be exploited into the standard collaborative filtering model:\n",
    "\n",
    "$$\\hat{r}_{i, j}=\\frac{\\sum_{u_{k} \\in F\\left(u_{i}\\right)} r_{k, j} \\cdot \\operatorname{Sim}\\left(u_{i}, u_{k}\\right)}{\\sum_{u_{k} \\in F\\left(u_{i}\\right)} \\operatorname{Sim}\\left(u_{i}, u_{k}\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequence(user, loc):\n",
    "    \"\"\"Get the frequence for a user and a location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user: double\n",
    "        user id\n",
    "        \n",
    "    loc: double\n",
    "        location id\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freq: integer\n",
    "        the frequency with which the user visited loc\n",
    "        0 if he didn't visit the loc\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        freq = df_frequencies[(df_frequencies.userid==user) & (df_frequencies.placeid==loc)].freq.values[0]\n",
    "    except:\n",
    "        # if the user did not visited the location\n",
    "        freq = 0\n",
    "    \n",
    "    return freq\n",
    "\n",
    "\n",
    "def prediction(user, loc, list_users=None):\n",
    "    \"\"\"Compute the prediction for an user and a location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user: double\n",
    "        user id\n",
    "        \n",
    "    loc: double\n",
    "        location id\n",
    "        \n",
    "    list_users: list on integers\n",
    "        list of users id for which we have some info\n",
    "        (later, those users will be users in the train set)\n",
    "        default None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r_hat: integer scoe of collaborative filtering\n",
    "    \n",
    "    \"\"\"\n",
    "    # list of user's friends\n",
    "    friends = dict_friends[user].tolist()\n",
    "    \n",
    "    if list_users is not None:\n",
    "        # select only friends that are in list_users\n",
    "        friends = np.intersect1d(friends, list_users)\n",
    "    \n",
    "    num = np.sum([get_frequence(x, loc)*social_similarity(user, x)\n",
    "                  for x in friends])\n",
    "    den = np.sum([social_similarity(user, x)\n",
    "                  for x in friends])\n",
    "    \n",
    "    if den!=0:\n",
    "        r_hat = num/den\n",
    "    else:\n",
    "        r_hat = 0\n",
    "    \n",
    "    return r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 41.6 ms, total: 205 ms\n",
      "Wall time: 115 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36171055022503246"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prediction(user=76, loc=9542)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Warning : the social similarity can be equal to 0 if the intersection of the set of friends of 2 users is empty. It can be a problem for the computation of the score.\n",
    "- Because we can divide by 0 if the social similarity of a user and all his friends is null. (no friends in common).\n",
    "- if the denominateur is equal to 0, we set to 0 the score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the prediction $\\hat{r}_{i,j}$ into a probability as follows:\n",
    "\n",
    "$$\\hat{p}_{i, j}=\\frac{\\hat{r}_{i, j}}{\\max _{l_{j} \\in L \\backslash L_{i}}\\left\\{\\hat{r}_{i, j}\\right\\}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vc-OO_z0XH-B"
   },
   "outputs": [],
   "source": [
    "def proba(user, loc, df_user_locations=df_user_locations):\n",
    "    \"\"\"Compute the probability for an user and a location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user: double\n",
    "        user id\n",
    "        \n",
    "    loc: double\n",
    "        location id\n",
    "        \n",
    "    list_locations: list of integer\n",
    "        list of locations id visited by the user\n",
    "        if None, then taken from df_user_locations.loc[user, 'list_locations']\n",
    "        default is None\n",
    "        \n",
    "    df_user_locations: pandas DataFrame\n",
    "        dataframe that associates for each user the list of the successive locations he visited\n",
    "        index: userid\n",
    "        columns = [list_locations]\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    p_hat: integer\n",
    "    \n",
    "    \"\"\"\n",
    "    # list of user's friends\n",
    "    friends = dict_friends[user]\n",
    "    \n",
    "    # select only friends that are in the df_user_locations\n",
    "    list_users = list(df_user_locations.index)\n",
    "    friends = np.intersect1d(friends, list_users)\n",
    "    \n",
    "    # set of all the locations visited by user's frieds\n",
    "    all_loc = set(df_user_locations.loc[friends].list_locations.explode())\n",
    "    \n",
    "    try:\n",
    "        # list of locations visited by the user\n",
    "        user_loc = set(df_user_locations.loc[user, 'list_locations'])\n",
    "    except:\n",
    "        # the user is not in df_user_locations\n",
    "        user_loc = []\n",
    "    \n",
    "    # location the user did not visit \n",
    "    possible_loc = all_loc.difference(user_loc)\n",
    "    \n",
    "    den = np.max([prediction(user, l, list_users) for l in possible_loc])\n",
    "    #den = np.max(list(map(lambda l: prediction(user, l, list_users), possible_loc)))\n",
    "    \n",
    "    if den != 0 :\n",
    "        p_hat = prediction(user,loc)/den\n",
    "    else:\n",
    "        p_hat = 0\n",
    "    \n",
    "    return p_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a problem of scalability with computing the probability, because the set $L \\setminus L_i$ is very big (more than 2 millions location).\n",
    "- But $\\hat{r}_{i, j}$ is actually null if none of the friends of user i visited location j.\n",
    "- So we can restrain L to the set of locations the friends of i visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eisv-98vHL84",
    "outputId": "f2b313a1-66d0-4476-aacc-a45e6db9b904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 630 ms, total: 17.6 s\n",
      "Wall time: 17.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2705840302044074"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "proba(user=76, loc=9542)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBcSD2k3XH-D"
   },
   "source": [
    "## 5. Generate & Test Recommendations\n",
    "\n",
    "Compute a recommandation score as follow:\n",
    "\n",
    "$$\\hat{s}_{i, j}=\\frac{\\hat{p}_{i, j}+p\\left(l_{i} | L_{u}\\right)}{2}$$\n",
    "\n",
    "This score given in equation 5 is the average of the collaborative score $\\hat{p}_{i, j}$ and the geographic probability $p\\left(l_{i} | L_{u}\\right)$. It gives a probability user i will like location j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q9Eb-d7HL9C"
   },
   "outputs": [],
   "source": [
    "def score(user, loc, df_user_locations=df_user_locations, method='sum'):\n",
    "    \"\"\"Generate a recommendation score for a given user and a location.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user: integer\n",
    "        user id\n",
    "        \n",
    "    loc: integer\n",
    "        location id\n",
    "        \n",
    "    df_user_locations: pandas DataFrame\n",
    "        dataframe that associates for each user the list of the successive locations he visited\n",
    "        index: userid\n",
    "        columns = [list_locations]\n",
    "        \n",
    "    method: string\n",
    "        the method used to compute the score ('sum', 'product')\n",
    "        default is 'sum'\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score: integer\n",
    "        the recommandation score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    p_hat = proba(user=user,\n",
    "                  loc=loc,\n",
    "                  df_user_locations=df_user_locations)\n",
    "\n",
    "    p = geo_proba(user=user,\n",
    "                  new_loc=loc,\n",
    "                  df_user_locations=df_user_locations)\n",
    "    \n",
    "    if method=='sum':\n",
    "        score = (p_hat + p)/2\n",
    "    elif method=='product':\n",
    "        score = p_hat * p\n",
    "    else:\n",
    "        raise ValueError('Unkown value for parameter method.')\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxp95iDyHL9P",
    "outputId": "05f7d1a7-1c34-4f37-b9a4-f0892fb48712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 378 ms, total: 17.4 s\n",
      "Wall time: 17.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13711081206075498"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "score(user=76, loc=9542)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to determine a list of locations that a given user is prone to like.\n",
    "To do so, given an user id, we return a list of $n$ locations with the best scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AZhRhmsXH-G"
   },
   "outputs": [],
   "source": [
    "def best_locations(user, n_locations=10, possible_loc='friends',\n",
    "                   df_user_locations=df_user_locations):\n",
    "    \"\"\"Generate the n best location for given user\n",
    "    among the ones he did not visited yet.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user: integer\n",
    "      user id \n",
    "    \n",
    "    n_locations: integer\n",
    "      number of recommendations to return\n",
    "      default is 10\n",
    "    \n",
    "    possible_loc: string\n",
    "      either 'all' or 'friends'\n",
    "      if 'all', searches among all locations in locations_df\n",
    "      if 'friends', searches among all locations friends_visited\n",
    "      default is 'friends'\n",
    "      \n",
    "    df_user_locations: pandas DataFrame\n",
    "        dataframe that associates for each user the list of the successive locations he visited\n",
    "        index: userid\n",
    "        columns = [list_locations]\n",
    "    \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    list_scores: list of tuples (location id, score)\n",
    "        orderer from largest score to smallest score\n",
    "    \n",
    "    \"\"\"\n",
    "    # list of tuples (location, score)\n",
    "    list_scores = [] \n",
    "    \n",
    "    try:\n",
    "        user_loc = set(df_user_locations.loc[user, 'list_locations'])\n",
    "    except:\n",
    "        user_loc = []\n",
    "    \n",
    "    if possible_loc=='all':\n",
    "        # set of possible locations\n",
    "        possible_loc_set = set(locations_df.id).difference(user_loc)\n",
    "    elif possible_loc=='friends':\n",
    "        # select only user's friends that are in the df_user_locations\n",
    "        friends = np.intersect1d(dict_friends[user], list(df_user_locations.index))\n",
    "        # set of locations visited by user's friends\n",
    "        possible_loc_set = set(df_user_locations.loc[friends].list_locations.explode())\n",
    "    else:\n",
    "        raise ValueError('Unkown value for parameter possible_loc.')\n",
    "        \n",
    "    print('number of possible locations:', len(possible_loc_set))\n",
    "    \n",
    "    for loc in tqdm(possible_loc_set):\n",
    "        s = score(user=user,\n",
    "                  loc=int(loc),\n",
    "                  df_user_locations=df_user_locations)\n",
    "        tup = (loc, s)\n",
    "        list_scores.append(tup)\n",
    "        \n",
    "    # sort list of scores from biggest to smallest score\n",
    "    list_scores.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return list_scores[:n_locations]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiZrIMD9HL9S",
    "outputId": "2bc0ac2e-8e3e-48c3-9115-3045642357af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible locations: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:18<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 20 should visit location 348429\n",
      "All 10 best locations for user 20:\n",
      " [(348429, 0.002055014476423036), (383982, 0.0017065745354810874), (206730, 0.0016279814675760926), (319729, 0.0014922199125443354), (26372, 0.0014728996770382997), (38629, 0.0013866008058440037), (94504, 0.0012541430906971717), (91672, 0.0007498056197647734), (85024, 0.0007408384578650412), (49309, 0.0007345378936888794)]\n",
      "CPU times: user 21.8 s, sys: 632 ms, total: 22.4 s\n",
      "Wall time: 18.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "user = 20\n",
    "n_locations = 10\n",
    "best_loc = best_locations(user=user, n_locations=n_locations)\n",
    "print(\"User %d should visit location %d\" %(user, best_loc[0][0]))\n",
    "print(\"All %d best locations for user %d:\\n\" %(n_locations, user), best_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise framework\n",
    "\n",
    "Documentation: https://surprise.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load df_frequencies into the Surprise framework with the load_from_df() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of values of frequencies \n",
    "reader = Reader(rating_scale=(0, 10)) \n",
    "\n",
    "# The columns of df_frequencies must correspond to\n",
    "# the user id, the place id and frequencies, in that order.\n",
    "data = Dataset.load_from_df(df_frequencies, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the train_test_split function to divide df_frequencies into a 75% train set and a 25% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.98 s, sys: 277 ms, total: 4.25 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainset, testset = train_test_split(data=data,\n",
    "                                     test_size=.25,\n",
    "                                     random_state=42, # seed\n",
    "                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 81003)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.all_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: the function train_test_split reinitialized userid to start from 0, as explained in the documentation (https://surprise.readthedocs.io/en/stable/FAQ.html#raw-inner-note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080245"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve initial id (raw id) of new id (inner id) 0\n",
    "trainset.to_raw_uid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEvIzG57HL9W"
   },
   "source": [
    "Create a GSLR class (i.e. Geographical Social Location Recommendation) and populate a fit() and a estimate() functions.\n",
    "\n",
    "When we evaluate our model with the test() method that uses the estimate() function, our aim is to estimate the rate given by the user for each location he visited, only using the informations that we have in the train set (we proceed as if all the data in the test set were non-existent).\n",
    "Hence, for a given user, in order to compute a score for a given location, we can only compute the score based on the locations he visited that are in the train set and on his friends that are in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 291,
     "status": "error",
     "timestamp": 1586477216107,
     "user": {
      "displayName": "Adriano Del Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiEUAmseN7ggLcZzboWv2nKxAAeo9LggrzomFI=s64",
      "userId": "04789708836990367239"
     },
     "user_tz": -120
    },
    "id": "QsTRFk6_XeOq",
    "outputId": "835b0c7e-d0db-4894-dd58-012edb236542"
   },
   "outputs": [],
   "source": [
    "class GSLR(AlgoBase):\n",
    "    def __init__(self): \n",
    "        AlgoBase.__init__(self)\n",
    "        \n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        dict_locations = defaultdict(list)\n",
    "        \n",
    "        for (u,i,_) in trainset.all_ratings():\n",
    "            # retrieve raw id\n",
    "            user = self.trainset.to_raw_uid(u)\n",
    "            loc = self.trainset.to_raw_iid(i)\n",
    "            # append location to the user's list\n",
    "            dict_locations[user].append(loc)\n",
    "\n",
    "        dict_locations = dict(dict_locations) # contains the users locations in the trainset\n",
    "\n",
    "        # recompute df_users_locations restricted to the train set\n",
    "        train_data = {'userid': list(dict_locations.keys()),\n",
    "                      'list_locations': list(dict_locations.values())}\n",
    "        self.df_user_locations_train = pd.DataFrame(data=train_data).set_index('userid')\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        # retrieve raw id for user and location\n",
    "        try:\n",
    "            # user is also in trainset\n",
    "            user = self.trainset.to_raw_uid(u)\n",
    "        except:\n",
    "            # user is not in trainset\n",
    "            user = int(re.findall(r'\\d+', u)[0]) #int(u.replace('UKN__',''))\n",
    "            \n",
    "        try:\n",
    "            # location is also in trainset\n",
    "            loc = self.trainset.to_raw_iid(i)\n",
    "        except:\n",
    "            # location is not in trainset\n",
    "            loc = int(re.findall(r'\\d+', i)[0]) #int(i.replace('UKN__',''))\n",
    "            \n",
    "        # compute score\n",
    "        s = score(user=user,\n",
    "                  loc=int(loc),\n",
    "                  df_user_locations=self.df_user_locations_train,\n",
    "                  method='sum')\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = GSLR().fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cedricallain/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/cedricallain/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/cedricallain/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 2267230    item: 3766604    r_ui = 1.24   est = 10.00   {'was_impossible': False}\n",
      "user: 454732     item: 978531     r_ui = 1.24   est = 0.00   {'was_impossible': False}\n",
      "user: 56547      item: 93066      r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 276330     item: 171535     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 1027964    item: 110444     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 260300     item: 124349     r_ui = 0.00   est = 0.03   {'was_impossible': False}\n",
      "user: 2168073    item: 300003     r_ui = 0.00   est = 0.03   {'was_impossible': False}\n",
      "user: 2116191    item: 52298      r_ui = 1.24   est = 0.34   {'was_impossible': False}\n",
      "user: 1945926    item: 501739     r_ui = 0.00   est = 0.25   {'was_impossible': False}\n",
      "user: 2134209    item: 5729168    r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 1035711    item: 79467      r_ui = 0.00   est = 0.06   {'was_impossible': False}\n",
      "user: 2638123    item: 214191     r_ui = 0.00   est = 0.08   {'was_impossible': False}\n",
      "user: 2295238    item: 685077     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 151302     item: 57955      r_ui = 1.24   est = 0.01   {'was_impossible': False}\n",
      "user: 1397507    item: 24110      r_ui = 1.24   est = 0.01   {'was_impossible': False}\n",
      "user: 2373569    item: 1226698    r_ui = 0.00   est = 10.00   {'was_impossible': False}\n",
      "user: 2299920    item: 7069883    r_ui = 0.00   est = 0.02   {'was_impossible': False}\n",
      "user: 2368432    item: 19166      r_ui = 1.24   est = 0.00   {'was_impossible': False}\n",
      "user: 2344175    item: 9669       r_ui = 0.00   est = 0.01   {'was_impossible': False}\n",
      "user: 2340772    item: 418985     r_ui = 0.00   est = 0.01   {'was_impossible': False}\n",
      "CPU times: user 8min 46s, sys: 20.2 s, total: 9min 6s\n",
      "Wall time: 9min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# number of data to keep in testset\n",
    "n_test = 20\n",
    "# reduce the testset: randomly pick n_test observations from testset\n",
    "testset_reduced = random.choices(testset, k=n_test)\n",
    "# make predictions\n",
    "predictions = algo.fit(trainset).test(testset_reduced, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall\n",
    "\n",
    "We now implement a precision/recall function.\n",
    "We reproduced this function: https://github.com/NicolasHug/Surprise/blob/master/examples/precision_recall_at_k.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjr4lUIGIrz0"
   },
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=0):\n",
    "    \"\"\"Return precision and recall at k metrics for each user.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: list of predictions\n",
    "        [(uid, iid, r_ui, est, details), ...]\n",
    "    \n",
    "    k: integer\n",
    "        number of metrics\n",
    "        \n",
    "    threshold: integer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precisions, recalls: integers\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r > threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est > threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r > threshold) and (est > threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means(precisions, recalls):\n",
    "    \"\"\" Compute average precision, average recall and average F1 score\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    precisions, recalls: dictionaries {user id: precision or recall}\n",
    "        outputs of function precision_recall_at_k() \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    avg_prec, avg_rec, avg_F1\n",
    "    \n",
    "    \"\"\"\n",
    "    precisions = list(precisions.values())\n",
    "    recalls = list(recalls.values())\n",
    "    \n",
    "    avg_prec, avg_rec, avg_F1 = np.mean([np.array([prec, rec, 2*prec*rec/(prec+rec+1e-16)])\n",
    "                                         for (prec, rec) in zip(precisions, recalls)],\n",
    "                                        axis=0)\n",
    "    return avg_prec, avg_rec, avg_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision: 0.20 \n",
      "average recall: 1.00 \n",
      "F1 score: 0.200000\n",
      "CPU times: user 11 ms, sys: 2.58 ms, total: 13.5 ms\n",
      "Wall time: 16.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# compute precision and recall for all users in test set\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=0)\n",
    "\n",
    "# compute average precision, average recall and average F1 score\n",
    "avg_prec ,avg_rec, avg_F1 = compute_means(precisions, recalls)\n",
    "print('average precision: %.2f \\naverage recall: %.2f \\nF1 score: %2f' %(avg_prec, avg_rec, avg_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwjXyctdYBxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "Fold 1 - precision: 0.50 - recall: 0.90 - F1: 0.400000\n",
      "Fold  2\n",
      "Fold 2 - precision: 0.10 - recall: 1.00 - F1: 0.100000\n",
      "Fold  3\n",
      "Fold 3 - precision: 0.20 - recall: 1.00 - F1: 0.200000\n",
      "CPU times: user 2min 33s, sys: 7.63 s, total: 2min 40s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "algo = GSLR()\n",
    "\n",
    "c = 0\n",
    "for trainset, testset in kf.split(data):\n",
    "    c += 1\n",
    "    \n",
    "    # train and test algorithm.\n",
    "    algo = algo.fit(trainset)\n",
    "    \n",
    "    # reduce the testset: randomly pick n_test observations from testset\n",
    "    testset_reduced = random.choices(testset, k=n_test)\n",
    "    predictions = algo.test(testset_reduced, verbose=False)\n",
    "    \n",
    "    # compute precisions and recalls\n",
    "    precisions, recalls = precision_recall_at_k(predictions)\n",
    "    \n",
    "    # Compute and print average recall precision and F1 score\n",
    "    avg_prec ,avg_rec, avg_F1 = compute_means(precisions, recalls)\n",
    "    \n",
    "    print('Fold %d - precision: %.2f - recall: %.2f - F1: %2f' %(c, avg_prec, avg_rec, avg_F1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = GSLR().fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default prediction\n",
    "Method: take the mean of all rates in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default prediction: 0.37832665373237223\n"
     ]
    }
   ],
   "source": [
    "default_pred = algo.default_prediction()\n",
    "print(\"Default prediction:\", default_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector of \"predictions\" by adding the default prediction value to every tuple of test set\n",
    "default_pred_list = [(*x, default_pred, {'was_impossible': False}) for x in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision: 0.18 \n",
      "average recall: 0.98 \n",
      "F1 score: 0.237900\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# compute average precision, average recall and F1 score\n",
    "precisions, recalls = precision_recall_at_k(default_pred_list, k=10, threshold=0)\n",
    "\n",
    "avg_prec ,avg_rec, avg_F1 = compute_means(precisions, recalls)\n",
    "\n",
    "print('average precision: %.2f \\naverage recall: %.2f \\nF1 score: %2f' %(avg_prec, avg_rec, avg_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN algorithm\n",
    "\n",
    "Method: for a given user and a given location, identify the $k$ nearest neighbours of the user (defined as the users, among his friends, that have the biggest social similarity), compute the average rating of the location given by the neighbours, and use the average rating to predict the rating that the user will give to the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(AlgoBase):\n",
    "    def __init__(self, k): \n",
    "        AlgoBase.__init__(self)\n",
    "        self.k = k\n",
    "        \n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        # recompute checkins_df restricted to the train set\n",
    "        checkins_df_train = {'userid': [], 'placeid': []}\n",
    "        \n",
    "        for (u,i,_) in trainset.all_ratings():\n",
    "            # retrieve raw id\n",
    "            user = self.trainset.to_raw_uid(u)\n",
    "            loc = self.trainset.to_raw_iid(i)\n",
    "            # append user and location in checkins_df_train\n",
    "            checkins_df_train['userid'].append(user)\n",
    "            checkins_df_train['placeid'].append(loc)\n",
    "        \n",
    "        self.checkins_df_train = pd.DataFrame(data=checkins_df_train)\n",
    "        \n",
    "        # list of unique users in the train set\n",
    "        self.train_users = set(checkins_df_train['userid'])\n",
    "        \n",
    "        # recompute df_frequencies restricted to the train set\n",
    "        df_frequencies_train = self.checkins_df_train.groupby(['userid', 'placeid']).agg({'placeid': 'count'})\n",
    "        df_frequencies_train.columns = ['freq']\n",
    "        df_frequencies_train.reset_index(inplace=True)\n",
    "        df_frequencies_train.freq = normalize_serie(df_frequencies_train.freq)\n",
    "        \n",
    "        self.df_frequencies_train = df_frequencies_train\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def find_knn(self, user):\n",
    "        \"\"\"Find the k nearest neighbours of user\n",
    "        \n",
    "        The k nearest neighbours of the user are defined as the users,\n",
    "        among his friends, that have the biggest social similarity with him.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        user: integer\n",
    "            user id\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        knn: list of integers\n",
    "            list of users id (the knn of input user)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        friends = dict_friends[user].tolist()\n",
    "        # restriction to the friends in train set\n",
    "        friends = np.intersect1d(friends, list(self.train_users))\n",
    "        \n",
    "        n_friends = len(friends)\n",
    "        \n",
    "        if n_friends==0:\n",
    "            return None\n",
    "        elif n_friends <= self.k:\n",
    "            return friends\n",
    "        \n",
    "        # compute social similarity \n",
    "        simi_list = [(f, social_similarity(user, f)) for f in friends]\n",
    "        # sort his friends by social similarity\n",
    "        simi_list.sort(key = lambda x: x[1], reverse=True)\n",
    "        # get the k nearest friends of user\n",
    "        knn = simi_list[:self.k]\n",
    "        \n",
    "        return knn\n",
    "    \n",
    "    \n",
    "    def knn_score(self, user, loc):\n",
    "        \"\"\"Compute knn score for a user.\n",
    "        \n",
    "        The knn score is defined as the average rating of the location\n",
    "        given by the k neighbours of the user.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        user: integer\n",
    "            user id\n",
    "            \n",
    "        loc: interger\n",
    "            location id\n",
    "            \n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        score: integer\n",
    "        \n",
    "        \"\"\"\n",
    "        # get neighbours of the users\n",
    "        nn = self.find_knn(user)\n",
    "        \n",
    "        if nn is None:\n",
    "            return 0\n",
    "        \n",
    "        df_frequencies_nn = self.df_frequencies_train.copy()\n",
    "        # filter on user's friends\n",
    "        df_frequencies_nn = df_frequencies_nn[df_frequencies_nn.userid.isin(nn)]\n",
    "        # filter on location\n",
    "        df_frequencies_nn = df_frequencies_nn[df_frequencies_nn.placeid == loc]\n",
    "        \n",
    "        # if none of user's friend visited the location, return 0\n",
    "        if df_frequencies_nn.shape[0]==0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = np.mean(df_frequencies_nn.freq)\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        # retrieve raw id for user and location\n",
    "        try:\n",
    "            # user is also in trainset\n",
    "            user = self.trainset.to_raw_uid(u)\n",
    "        except:\n",
    "            # user is not in trainset\n",
    "            user = int(re.findall(r'\\d+', u)[0]) #int(u.replace('UKN__',''))\n",
    "            \n",
    "        try:\n",
    "            # location is also in trainset\n",
    "            loc = self.trainset.to_raw_iid(i)\n",
    "        except:\n",
    "            # location is not in trainset\n",
    "            loc = int(re.findall(r'\\d+', i)[0]) #int(i.replace('UKN__',''))\n",
    "            \n",
    "        # compute score\n",
    "        s = self.knn_score(user=user, loc=int(loc))\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with 5 neighbours\n",
    "algo = KNN(5).fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 663122     item: 765162     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 2134894    item: 103109     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 265642     item: 298845     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 337431     item: 567547     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 104320     item: 31784      r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 360408     item: 750204     r_ui = 1.24   est = 0.00   {'was_impossible': False}\n",
      "user: 67383      item: 830557     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 2233037    item: 666449     r_ui = 1.24   est = 0.00   {'was_impossible': False}\n",
      "user: 2124639    item: 294215     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 223661     item: 52200      r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 2173859    item: 2243905    r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 2140934    item: 26134      r_ui = 1.24   est = 0.00   {'was_impossible': False}\n",
      "user: 2562646    item: 864219     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 4535       item: 33646      r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 2296362    item: 637723     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 117713     item: 32465      r_ui = 3.58   est = 0.00   {'was_impossible': False}\n",
      "user: 397383     item: 21655      r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 74929      item: 401289     r_ui = 1.24   est = 0.00   {'was_impossible': False}\n",
      "user: 2300380    item: 6924353    r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "user: 140666     item: 556187     r_ui = 0.00   est = 0.00   {'was_impossible': False}\n",
      "average precision: 1.00 \n",
      "average recall: 0.75 \n",
      "F1 score: 0.750000\n",
      "CPU times: user 8.22 s, sys: 374 ms, total: 8.59 s\n",
      "Wall time: 8.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# reduce the testset: randomly pick n_test observations from testset\n",
    "testset_reduced = random.choices(testset, k=n_test)\n",
    "\n",
    "# make predictions\n",
    "predictions = algo.fit(trainset).test(testset_reduced, verbose=True)\n",
    "\n",
    "# compute precision and recall for all users in test set\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=0)\n",
    "\n",
    "# compute average precision, average recall and average F1 score\n",
    "avg_prec ,avg_rec, avg_F1 = compute_means(precisions, recalls)\n",
    "print('average precision: %.2f \\naverage recall: %.2f \\nF1 score: %2f' %(avg_prec, avg_rec, avg_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.df_frequencies_train.freq.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: algo.df_frequencies_train is full of 0 for frequency column. That exmplains the better score of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "project_lbsn_v6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
